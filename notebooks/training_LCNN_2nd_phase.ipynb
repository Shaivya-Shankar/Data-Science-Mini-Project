{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "import keras_tuner as kt  # Import Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def read_and_norm(dataset_path, type_,type):\n",
    "    with open('{}/{}/{}.txt'.format(dataset_path, type_,type), 'rb') as f:\n",
    "        matrix = [[float(x) for x in line.split()] for line in f]\n",
    "    matrix = np.array(matrix)\n",
    "    min_m = matrix.min().min()\n",
    "    max_m = matrix.max().max()\n",
    "    matrix = ((matrix - min_m) / (max_m - min_m))\n",
    "    return matrix\n",
    "\n",
    "def load_full_dataset(type_,dataset_path):\n",
    "\tclassification = np.loadtxt('{}/{}/classification.txt'.format(dataset_path, type_))\n",
    "\tclassification = np.array(classification).reshape(-1,1)\n",
    "\n",
    "\twith open('{}/{}/hr.txt'.format(dataset_path, type_), \"r\") as file:\n",
    "\t\thr = []\n",
    "\t\trighe_con_9_colonne = []\n",
    "\t\tfor indice, riga in enumerate(file):\n",
    "\t\t\tcolonne = riga.split()\n",
    "\t\t\tif len(colonne) == 9:\n",
    "\t\t\t\trighe_con_9_colonne.append(indice)\n",
    "\t\t\telse:\n",
    "\t\t\t\thr.append(colonne)\n",
    "\thr = [[float(string) for string in inner] for inner in hr]\n",
    "\thr = np.array(hr)\n",
    "\n",
    "\n",
    "\tshape = read_and_norm(dataset_path, type_,'shape')\n",
    "\tel = read_and_norm(dataset_path, type_,'el')\n",
    "\tdist = read_and_norm(dataset_path, type_,'dist')\n",
    "\n",
    "\tclassification = np.delete(classification, righe_con_9_colonne, 0)\n",
    "\tshape = np.delete(shape, righe_con_9_colonne, 0)\n",
    "\tel = np.delete(el, righe_con_9_colonne, 0)\n",
    "\tdist = np.delete(dist, righe_con_9_colonne, 0)\n",
    "\n",
    "\n",
    "\tdata_X = np.array([p for p in zip(shape, dist, el, hr)])\n",
    "\tdata_X = data_X.reshape(data_X.shape[0], data_X.shape[1], data_X.shape[2], 1)\n",
    "\n",
    "\treturn(data_X,classification)\n",
    "\n",
    "\n",
    "\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "# fit and evaluate a model\n",
    "def build_model(hp, trainX, trainy):\n",
    "\tn_outputs = trainy.shape[1]\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(filters=hp.Int('filters_1', min_value=4, max_value=16, step=4), \n",
    "\t\t\t\t\t kernel_size=(4, 1), \n",
    "\t\t\t\t\t input_shape=trainX.shape[1:], \n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Conv2D(filters=hp.Int('filters_2', min_value=2, max_value=8, step=2), \n",
    "\t\t\t\t\t kernel_size=(1, 3), \n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(hp.Int('dense_units_1', min_value=20, max_value=50, step=10), activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Dense(hp.Int('dense_units_2', min_value=20, max_value=50, step=10), activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
    "\tmodel.compile(loss='binary_crossentropy', \n",
    "\t\t\t\t  optimizer='adam', \n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "def evaluate_model_2dconv(trainX, trainy, testX, testy, save_model, model_path, file_path):\n",
    "\ttuner = RandomSearch(\n",
    "\t\tlambda hp: build_model(hp, trainX, trainy),\n",
    "\t\tobjective='val_accuracy',\n",
    "\t\tmax_trials=10,\n",
    "\t\texecutions_per_trial=1,\n",
    "\t\tdirectory='hyperparam_tuning',\n",
    "\t\tproject_name='2dconv_tuning'\n",
    "\t)\n",
    "\n",
    "\ttuner.search(trainX, trainy, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "\tbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\tmodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "\tes = EarlyStopping(monitor='val_accuracy',\n",
    "\t\t\t\t\t   mode='max',\n",
    "\t\t\t\t\t   patience=50,\n",
    "\t\t\t\t\t   restore_best_weights=True)\n",
    "\n",
    "\thistory = model.fit(trainX, trainy, epochs=300, batch_size=1, verbose=0, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=1, verbose=0)\n",
    "\n",
    "\tpred_label_ = model.predict(testX, batch_size=1, verbose=0)\n",
    "\n",
    "\tpred_label = [1. if x >= 0.5 else 0. for x in pred_label_]\n",
    "\tresults = pd.DataFrame({'Pred': pred_label, 'Prob': pred_label_.reshape(-1), 'True': testy.reshape(-1)})\n",
    "\n",
    "\tif save_model:\n",
    "\t\tmodel.save('{}/{}.keras'.format(model_path, file_path))\n",
    "\n",
    "\treturn accuracy, history\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\t#print(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\t#print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\treturn m\n",
    "\n",
    "\n",
    "\n",
    "results = {\"File\":[],\n",
    "\t\t   \"Scores\":[],\n",
    "\t\t   \"Best_Accuracy\":[]}\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=5):\n",
    "\tsave_model = 1\n",
    "\tmodel_path = '../models/2nd_phase'\n",
    "\n",
    "\tdirectory = \"../generated_data/training_folders\"\n",
    "\tfor file_path in tqdm(os.listdir(directory)):\n",
    "\t\tdataset_path = directory + \"/\" +file_path\n",
    "\t\t\t\n",
    "\t\t#load data\n",
    "\t\ttrainX, trainy = load_full_dataset('train',dataset_path)\n",
    "\t\ttestX, testy = load_full_dataset('test',dataset_path)\n",
    "\t\t#print(trainX.shape,trainy.shape,testX.shape,testy.shape)\n",
    "\t\t\n",
    "\t\t# repeat experiment\n",
    "\t\tscores = list()\n",
    "\t\tfor r in range(repeats):\n",
    "\t\t\tscore,history = evaluate_model_2dconv(trainX, trainy, testX, testy, save_model, model_path, file_path)\n",
    "\n",
    "\t\t\tscore = score * 100.0\n",
    "\t\t\t#print('>#%d: %.3f' % (r+1, score))\n",
    "\t\t\tscores.append(score)\n",
    "\n",
    "\t\t#print(score)\n",
    "\t\tbest_score = summarize_results(scores)\n",
    "\t\t\n",
    "\t\t# save it to dataframe\n",
    "\t\tresults[\"File\"].append(file_path)\n",
    "\t\tresults[\"Scores\"].append(scores)\n",
    "\t\tresults[\"Best_Accuracy\"].append(best_score)\n",
    "\t\tdf = pd.DataFrame(results)\n",
    "\t\tmax_accuracy_row = df.loc[df['Best_Accuracy'].idxmax()]\n",
    "\t\tprint(max_accuracy_row)\n",
    "\t\tdf.to_csv(\"../static/phase_2_results.csv\", header=False)\n",
    "\n",
    "run_experiment()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_minmax_file(dataset_path, type_, data_type, apply_noise=False, scaler=None):\n",
    "    \"\"\"\n",
    "    Reads a .txt file with numerical data line-by-line.\n",
    "    \n",
    "    For training (scaler is None):\n",
    "        - Optionally adds Gaussian noise (σ=0.4).\n",
    "        - Computes the min and max values of the data and then applies Min–Max scaling.\n",
    "        - Returns the scaled matrix and a dictionary with:\n",
    "              'minmax': (min_val, max_val)\n",
    "              \n",
    "    For test data (scaler provided):\n",
    "        - Does not add noise.\n",
    "        - Uses the learned min–max values to perform scaling.\n",
    "    \n",
    "    Returns:\n",
    "        scaled_matrix: The scaled data.\n",
    "        scaler_params: Dictionary with min–max parameters.\n",
    "        valid_indices: List of line indices that were successfully parsed.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_path, type_, f\"{data_type}.txt\")\n",
    "    matrix = []\n",
    "    valid_indices = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                values = [float(x) for x in line.strip().split()]\n",
    "                matrix.append(values)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid line {idx+1} in {file_path}: {e}\")\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    # Add Gaussian noise only for training (increase noise to 0.4)\n",
    "    if apply_noise:\n",
    "        matrix = matrix + np.random.normal(loc=0.0, scale=0.2, size=matrix.shape)\n",
    "    \n",
    "    if scaler is None:\n",
    "        # Compute min and max values for min-max scaling\n",
    "        min_val = matrix.min()\n",
    "        max_val = matrix.max()\n",
    "        if max_val - min_val == 0:\n",
    "            scaled = matrix\n",
    "        else:\n",
    "            scaled = (matrix - min_val) / (max_val - min_val)\n",
    "        scaler_params = {'minmax': (min_val, max_val)}\n",
    "    else:\n",
    "        min_val, max_val = scaler['minmax']\n",
    "        if max_val - min_val == 0:\n",
    "            scaled = matrix\n",
    "        else:\n",
    "            scaled = (matrix - min_val) / (max_val - min_val)\n",
    "        scaler_params = scaler  # unchanged for test\n",
    "    \n",
    "    return scaled, scaler_params, valid_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_minmax_hr(dataset_path, type_, apply_noise=False, scaler=None):\n",
    "    \"\"\"\n",
    "    Special processing for hr.txt:\n",
    "      - Skip lines that have exactly 9 columns.\n",
    "      - For valid lines, parse numerical data.\n",
    "    \n",
    "    For training (scaler is None):\n",
    "        - Optionally adds Gaussian noise.\n",
    "        - Computes the min and max values of the data and then applies Min–Max scaling.\n",
    "        - Returns the scaled data and a dictionary with:\n",
    "              'minmax': (min_val, max_val)\n",
    "    \n",
    "    For test data (scaler provided):\n",
    "        - Does not add noise.\n",
    "        - Uses the learned min–max values to perform scaling.\n",
    "    \n",
    "    Returns:\n",
    "        hr_scaled: The scaled HR data.\n",
    "        scaler_params: Dictionary with scaling parameters.\n",
    "        valid_indices: List of valid line indices.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_path, type_, \"hr.txt\")\n",
    "    hr_list = []\n",
    "    valid_indices = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            cols = line.strip().split()\n",
    "            # Skip lines with exactly 9 columns.\n",
    "            if len(cols) == 9:\n",
    "                continue\n",
    "            try:\n",
    "                values = [float(x) for x in cols]\n",
    "                hr_list.append(values)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid HR line {idx+1} in {file_path}: {e}\")\n",
    "    hr = np.array(hr_list)\n",
    "    \n",
    "    if apply_noise:\n",
    "        hr = hr + np.random.normal(loc=0.0, scale=0.2, size=hr.shape)\n",
    "    \n",
    "    if scaler is None:\n",
    "        min_val = hr.min()\n",
    "        max_val = hr.max()\n",
    "        if max_val - min_val == 0:\n",
    "            hr_scaled = hr\n",
    "        else:\n",
    "            hr_scaled = (hr - min_val) / (max_val - min_val)\n",
    "        scaler_params = {'minmax': (min_val, max_val)}\n",
    "    else:\n",
    "        min_val, max_val = scaler['minmax']\n",
    "        if max_val - min_val == 0:\n",
    "            hr_scaled = hr\n",
    "        else:\n",
    "            hr_scaled = (hr - min_val) / (max_val - min_val)\n",
    "        scaler_params = scaler\n",
    "    \n",
    "    return hr_scaled, scaler_params, valid_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dataset(dataset_path,type_, scalers=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the full dataset.\n",
    "\n",
    "    For each feature (shape, el, dist, hr):\n",
    "      - Reads the file.\n",
    "      - For training: adds Gaussian noise (σ=0.4), computes min–max scaling.\n",
    "      - For test: uses the learned min–max scaling parameters from training.\n",
    "    Computes the common valid indices and combines the features.\n",
    "    \n",
    "    Returns:\n",
    "        data_X: Combined 4D array (samples, features, timesteps, channels).\n",
    "        classification: Filtered classification labels.\n",
    "        scalers: Dictionary of scaling parameters for each feature (for training) or None (for test).\n",
    "    \"\"\"\n",
    "    # Load classification labels.\n",
    "    class_file = os.path.join(dataset_path, type_, \"classification.txt\")\n",
    "    classification = np.loadtxt(class_file).reshape(-1, 1)\n",
    "    classification_indices = list(range(len(classification)))\n",
    "    \n",
    "    # Use noise only for training.\n",
    "    apply_noise_flag = True if type_ == 'train' else False\n",
    "    \n",
    "    # Process HR using its special handler.\n",
    "    if scalers is None or 'hr' not in scalers:\n",
    "        hr_scaled, hr_scaler, hr_valid = read_and_minmax_hr(dataset_path, type_, apply_noise=apply_noise_flag, scaler=None)\n",
    "    else:\n",
    "        hr_scaled, hr_scaler, hr_valid = read_and_minmax_hr(dataset_path, type_, apply_noise=False, scaler=scalers['hr'])\n",
    "    \n",
    "    # Process other features: shape, el, dist.\n",
    "    feat_names = ['shape', 'el', 'dist']\n",
    "    feature_data = {}\n",
    "    feature_scalers = {}\n",
    "    feature_valid = {}\n",
    "    for feat in feat_names:\n",
    "        if scalers is None or feat not in scalers:\n",
    "            data, scaler_param, valid_idx = read_and_minmax_file(dataset_path, type_, feat, apply_noise=apply_noise_flag, scaler=None)\n",
    "        else:\n",
    "            data, scaler_param, valid_idx = read_and_minmax_file(dataset_path, type_, feat, apply_noise=False, scaler=scalers[feat])\n",
    "        feature_data[feat] = data\n",
    "        feature_scalers[feat] = scaler_param\n",
    "        feature_valid[feat] = valid_idx\n",
    "    \n",
    "    # Determine common valid indices among all features and classification.\n",
    "    common_valid = set(classification_indices)\n",
    "    common_valid = common_valid.intersection(set(hr_valid))\n",
    "    for feat in feat_names:\n",
    "        common_valid = common_valid.intersection(set(feature_valid[feat]))\n",
    "    common_valid = sorted(common_valid)\n",
    "    if len(common_valid) == 0:\n",
    "        raise ValueError(\"No common valid indices across all features!\")\n",
    "    \n",
    "    # Filter each array using the common valid indices.\n",
    "    classification = classification[common_valid]\n",
    "    hr_scaled = hr_scaled[common_valid]\n",
    "    for feat in feat_names:\n",
    "        feature_data[feat] = feature_data[feat][common_valid]\n",
    "\n",
    "    # Validate shapes: all features must match in sample count and timesteps.\n",
    "    def validate_feature_shapes(features_dict):\n",
    "        first_shape = None\n",
    "        #print(type_)\n",
    "        for name, arr in features_dict.items():\n",
    "            #print(name,arr.shape)\n",
    "            if first_shape is None:\n",
    "                first_shape = (arr.shape[0], arr.shape[1])\n",
    "            else:\n",
    "                if (arr.shape[0], arr.shape[1]) != first_shape:\n",
    "                    raise ValueError(f\"Shape mismatch in {name}: Expected {first_shape}, got {(arr.shape[0], arr.shape[1])}\")\n",
    "    validate_feature_shapes({feat: feature_data[feat] for feat in feat_names})\n",
    "    validate_feature_shapes({'hr': hr_scaled, **{feat: feature_data[feat] for feat in feat_names}})\n",
    "    \n",
    "    # Combine features along a new axis.\n",
    "    # New order: (shape, dist, el, hr)\n",
    "    data_X = np.stack([feature_data['shape'], feature_data['dist'], feature_data['el'], hr_scaled], axis=1)\n",
    "    # Reshape for CNN input: (samples, features, timesteps, channels)\n",
    "    data_X = data_X.reshape(data_X.shape[0], data_X.shape[1], data_X.shape[2], 1)\n",
    "    \n",
    "    # For training data, return the computed scalers; for test, return None.\n",
    "    if scalers is None:\n",
    "        scalers_out = {\n",
    "            'hr': hr_scaler,\n",
    "            'shape': feature_scalers['shape'],\n",
    "            'el': feature_scalers['el'],\n",
    "            'dist': feature_scalers['dist']\n",
    "        }\n",
    "    else:\n",
    "        scalers_out = None\n",
    "    return data_X, classification, scalers_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp, input_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(filters=hp.Int('filters_1', min_value=4, max_value=16, step=4), \n",
    "\t\t\t\t\t kernel_size=(4, 1), \n",
    "\t\t\t\t\t input_shape=input_shape, \n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Conv2D(filters=hp.Int('filters_2', min_value=2, max_value=8, step=2), \n",
    "\t\t\t\t\t kernel_size=(1, 3), \n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(hp.Int('dense_units_1', min_value=20, max_value=50, step=10), activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Dense(hp.Int('dense_units_2', min_value=20, max_value=50, step=10), activation='relu'))\n",
    "\tmodel.add(Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\tmodel.compile(loss='binary_crossentropy', \n",
    "\t\t\t\t  optimizer='adam', \n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_optimal_threshold(model, X_val, y_val):\n",
    "    \"\"\"Find optimal threshold using ROC curve analysis\"\"\"\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n",
    "    # Calculate Youden's J statistic\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Calculate AUC\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    return optimal_threshold, fpr, tpr, roc_auc\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculate all metrics for a given threshold\"\"\"\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metrics = {\n",
    "        'confusion_matrix': cm,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': tp/(tp+fp) if (tp+fp) > 0 else 0,\n",
    "        'recall': tp/(tp+fn) if (tp+fn) > 0 else 0,\n",
    "        'f1': 2*tp/(2*tp + fp + fn) if (2*tp + fp + fn) > 0 else 0,\n",
    "        'accuracy': (tp + tn)/(tp + tn + fp + fn),\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc, threshold,file_name=\"test\"):\n",
    "    \"\"\"Plot ROC curve with optimal threshold marker\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.scatter(fpr[np.argmax(tpr - fpr)], tpr[np.argmax(tpr - fpr)], \n",
    "                color='red', label=f'Optimal Threshold ({threshold:.3f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'../static/roc_curves/phase_2/{file_name}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b426da8d4f44e3b84e15b97ced72a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step\n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Reloading Tuner from hyperparam_tuning/2dconv_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment():\n",
    "    save_model = 1\n",
    "    model_path = '../models/2nd_phase'\n",
    "\n",
    "    result_file_path = '../static/phase_2_results_32.csv'\n",
    "    #directory = \"../Archive/generated_data_achive/training_folders/BLAS910101\"\n",
    "    #directory = \"../dataset\"\n",
    "    # start the for loop for filename and string concat for the complete path\n",
    "    directory_ = \"../Archive/generated_data_achive/training_folders\"\n",
    "    for file_path in tqdm(os.listdir(directory_)):\n",
    "        dataset_path = directory_ + \"/\" +file_path\n",
    "    \n",
    "\n",
    "        try:\n",
    "            # Load and split data\n",
    "            trainX, trainy, scalers = load_full_dataset(dataset_path,'train')\n",
    "            #print(trainX.shape,trainy.shape)\n",
    "            # Split training data into train/validation\n",
    "            split_idx = int(len(trainX) * 0.8)\n",
    "            X_train, X_val = trainX[:split_idx], trainX[split_idx:]\n",
    "            y_train, y_val = trainy[:split_idx], trainy[split_idx:]\n",
    "            \n",
    "   \n",
    "            tuner = RandomSearch(\n",
    "                lambda hp: build_model(hp, trainX.shape[1:]),\n",
    "                objective='val_accuracy',\n",
    "                max_trials=10,\n",
    "                executions_per_trial=1,\n",
    "                directory='hyperparam_tuning',\n",
    "                project_name='2dconv_tuning'\n",
    "            )\n",
    "\n",
    "            tuner.search(trainX, trainy, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "            model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "            es = EarlyStopping(monitor='val_accuracy',\n",
    "                            mode='max',\n",
    "                            patience=50,\n",
    "                            restore_best_weights=True)\n",
    "\n",
    "            history = model.fit(trainX, trainy, epochs=300, batch_size=32, verbose=0, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "            if save_model:\n",
    "                model.save('{}/{}.keras'.format(model_path, file_path))\n",
    "            \n",
    "            # Find optimal threshold using ROC analysis\n",
    "            optimal_threshold, fpr, tpr, roc_auc = find_optimal_threshold(model, X_val, y_val)\n",
    "            plot_roc_curve(fpr, tpr, roc_auc, optimal_threshold,file_path)\n",
    "            \n",
    "            # Load and evaluate test data\n",
    "            testX, testy, _ = load_full_dataset(dataset_path,'test', scalers)\n",
    "        \n",
    "            y_test_pred = model.predict(testX)\n",
    "            \n",
    "            # Calculate metrics with optimal threshold\n",
    "            test_metrics = calculate_metrics(testy, y_test_pred, optimal_threshold)\n",
    "            # Print comprehensive results\n",
    "            # make test_metrics[\"confusion_matrix\"] as string\n",
    "            test_metrics[\"File\"] = file_path\n",
    "            test_metrics[\"confusion_matrix\"] = str(test_metrics[\"confusion_matrix\"]) \n",
    "            test_metrics = {key: [value] for key, value in test_metrics.items()}\n",
    "\n",
    "            result_df = pd.DataFrame(test_metrics)\n",
    "\n",
    "            #print(result_df.info())\n",
    "            # Check if the file exists\n",
    "            if not os.path.exists(result_file_path):\n",
    "                # If it doesn't exist, save the current DataFrame\n",
    "                result_df.to_csv(result_file_path, index=False)\n",
    "            else:\n",
    "                # If it exists, read the existing file\n",
    "                existing_df = pd.read_csv(result_file_path)\n",
    "                \n",
    "                # Append the current DataFrame to the existing one\n",
    "                combined_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "                \n",
    "                # Save the combined DataFrame back to the file\n",
    "                combined_df.to_csv(result_file_path, index=False)\n",
    "        \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusion_matrix    [[488 164]\\n [126 504]]\n",
       "tp                                      504\n",
       "tn                                      488\n",
       "fp                                      164\n",
       "fn                                      126\n",
       "precision                          0.754491\n",
       "recall                                  0.8\n",
       "f1                                 0.776579\n",
       "accuracy                           0.773791\n",
       "threshold                          0.508202\n",
       "roc_auc                            0.850767\n",
       "File                             KUHL950101\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../static/phase_2_results.csv')\n",
    "# Find the row with the highest accuracy\n",
    "top_accuracy_row = df.loc[df['accuracy'].idxmax()]\n",
    "\n",
    "top_accuracy_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
