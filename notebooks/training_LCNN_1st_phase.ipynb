{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model using L_CNN Architecture\n",
    "\n",
    "This notebook covers the first phase of training the models using various Hydropathy scales\n",
    "The code in `generate_hr.ipynb` and `code/generate_hr.py` are used to generate multiple Hr files based on varius hydropathy scale\n",
    "\n",
    "Following that In this notebook, we use all the generated Hr files to train the model and check accuracy as well as save the results.\n",
    "\n",
    "We are using the already given L_CNN architecture without tuning to train models. There are 28 Hr scales thus we will have 28 model trained and their accuracy will be saved in a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the perfect structure\n",
    "\n",
    "First let's create the training and testing data strcuture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(\"../generated_data/train/\", exist_ok=True)\n",
    "\n",
    "#for file_path in tqdm(os.listdir(directory)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../generated_data/training_folders/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# read text file as numpy array\n",
    "\n",
    "# training files\n",
    "filename = '../dataset/train/dataset.txt'\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "filename = \"../dataset/train/shape.txt\"\n",
    "sh = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/train/dist.txt\"\n",
    "dist = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/train/el.txt\"\n",
    "el = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/train/classification.txt\"\n",
    "classification = pd.read_csv(filename,delimiter = \" \", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# read text file as numpy array\n",
    "\n",
    "# test files\n",
    "filename = '../dataset/test/dataset.txt'\n",
    "dataset_test = pd.read_csv(filename)\n",
    "\n",
    "filename = \"../dataset/test/shape.txt\"\n",
    "sh_test = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/test/dist.txt\"\n",
    "dist_test = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/test/el.txt\"\n",
    "el_test = pd.read_csv(filename,delimiter = \" \", header=None)\n",
    "\n",
    "filename = \"../dataset/test/classification.txt\"\n",
    "classification_test = pd.read_csv(filename,delimiter = \" \", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613129f960b04158ba495f5e0dec2937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b964aef1f94c19aed6652faf690e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Creation Done\n"
     ]
    }
   ],
   "source": [
    "# for training set\n",
    "directory = \"../generated_data/train/Hr\"\n",
    "for file_path in tqdm(os.listdir(directory)):\n",
    "    folder_name = file_path.split(\".\")[0]\n",
    "    #print(folder_name)\n",
    "    os.makedirs(f\"../generated_data/training_folders/{folder_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../generated_data/training_folders/{folder_name}/train\", exist_ok=True)\n",
    "    os.makedirs(f\"../generated_data/training_folders/{folder_name}/test\", exist_ok=True)\n",
    "\n",
    "    # read hr file\n",
    "    hr = pd.read_csv(directory + \"/\" + file_path,delimiter = \" \", header=None)\n",
    "\n",
    "    # read\n",
    "\n",
    "    # for training set--------------------------------\n",
    "    # save dataset.txt in all the train folders\n",
    "    dataset.to_csv(f'../generated_data/training_folders/{folder_name}/train/dataset.txt', sep=',', index=False)\n",
    "\n",
    "    # save shape.txt in all the train folders\n",
    "    sh.to_csv(f'../generated_data/training_folders/{folder_name}/train/shape.txt', sep=' ', header=None, index=False)\n",
    "\n",
    "    # save dist.txt in all the train folders\n",
    "    dist.to_csv(f'../generated_data/training_folders/{folder_name}/train/dist.txt', sep=' ',header=None, index=False)\n",
    "    \n",
    "    # save el.txt in all the train folders\n",
    "    el.to_csv(f'../generated_data/training_folders/{folder_name}/train/el.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "    # save hr.txt in all the train folders\n",
    "    hr.to_csv(f'../generated_data/training_folders/{folder_name}/train/hr.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "    # save classification.txt in all the train folders\n",
    "    classification.to_csv(f'../generated_data/training_folders/{folder_name}/train/classification.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "\n",
    "# for training set\n",
    "directory_test = \"../generated_data/test/Hr_modified_gaussian_Power_transformed\"\n",
    "for file_path in tqdm(os.listdir(directory_test)):\n",
    "    folder_name = file_path.split(\".\")[0]\n",
    "    os.makedirs(f\"../generated_data/training_folders/{folder_name}/test\", exist_ok=True)\n",
    "\n",
    "    # read hr file\n",
    "    hr_test = pd.read_csv(directory_test + \"/\" + file_path,delimiter = \" \", header=None)\n",
    "\n",
    "    # for test set--------------------------------\n",
    "    # save dataset.txt in all the train folders\n",
    "    dataset_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/dataset.txt', sep=',', index=False)\n",
    "\n",
    "    # save shape.txt in all the train folders\n",
    "    sh_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/shape.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "    # save dist.txt in all the train folders\n",
    "    dist_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/dist.txt', sep=' ',header=None, index=False)\n",
    "    \n",
    "    # save el.txt in all the train folders\n",
    "    el_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/el.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "    # save hr.txt in all the train folders\n",
    "    hr_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/hr.txt', sep=' ',header=None, index=False)\n",
    "\n",
    "    # save classification.txt in all the train folders\n",
    "    classification_test.to_csv(f'../generated_data/training_folders/{folder_name}/test/classification.txt',header=None, sep=' ', index=False)\n",
    "\n",
    "print(\"Folder Creation Done\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on various Hr files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archive code\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def read_and_norm(dataset_path, type_,type):\n",
    "    with open('{}/{}/{}.txt'.format(dataset_path, type_,type), 'rb') as f:\n",
    "        matrix = [[float(x) for x in line.split()] for line in f]\n",
    "    matrix = np.array(matrix)\n",
    "    min_m = matrix.min().min()\n",
    "    max_m = matrix.max().max()\n",
    "    matrix = ((matrix - min_m) / (max_m - min_m))\n",
    "    return matrix\n",
    "\n",
    "def load_full_dataset(type_,dataset_path):\n",
    "\tclassification = np.loadtxt('{}/{}/classification.txt'.format(dataset_path, type_))\n",
    "\tclassification = np.array(classification).reshape(-1,1)\n",
    "\n",
    "\twith open('{}/{}/hr.txt'.format(dataset_path, type_), \"r\") as file:\n",
    "\t\thr = []\n",
    "\t\trighe_con_9_colonne = []\n",
    "\t\tfor indice, riga in enumerate(file):\n",
    "\t\t\tcolonne = riga.split()\n",
    "\t\t\tif len(colonne) == 9:\n",
    "\t\t\t\trighe_con_9_colonne.append(indice)\n",
    "\t\t\telse:\n",
    "\t\t\t\thr.append(colonne)\n",
    "\thr = [[float(string) for string in inner] for inner in hr]\n",
    "\thr = np.array(hr)\n",
    "\n",
    "\n",
    "\tshape = read_and_norm(dataset_path, type_,'shape')\n",
    "\tel = read_and_norm(dataset_path, type_,'el')\n",
    "\tdist = read_and_norm(dataset_path, type_,'dist')\n",
    "\n",
    "\tclassification = np.delete(classification, righe_con_9_colonne, 0)\n",
    "\tshape = np.delete(shape, righe_con_9_colonne, 0)\n",
    "\tel = np.delete(el, righe_con_9_colonne, 0)\n",
    "\tdist = np.delete(dist, righe_con_9_colonne, 0)\n",
    "\n",
    "\n",
    "\tdata_X = np.array([p for p in zip(shape, dist, el, hr)])\n",
    "\tdata_X = data_X.reshape(data_X.shape[0], data_X.shape[1], data_X.shape[2], 1)\n",
    "\n",
    "\treturn(data_X,classification)\n",
    "\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model_2dconv(trainX, trainy, testX, testy, save_model, model_path, file_path):\n",
    "\tverbose, epochs, batch_size = 0, 300, 1\n",
    "\tn_outputs = trainy.shape[1]\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(filters=9, kernel_size=(4,1), input_shape=trainX.shape[1:],activation='relu'))\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\tmodel.add(Conv2D(filters=3, kernel_size=(1,3), activation='relu')) #(1,5)\n",
    "\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(30, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
    "\t#model.summary()\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\tes = EarlyStopping(monitor='val_accuracy',\n",
    "\t\t\t\t   mode='max',\n",
    "\t\t\t\t   patience=50,\n",
    "\t\t\t\t   restore_best_weights=True)\n",
    "\n",
    "\thistory = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=.2,callbacks=[es])#,callbacks=rlronp)#) #,callbacks=[es])\n",
    "\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\n",
    "\tpred_label_ = model.predict(testX,batch_size=batch_size, verbose=0)\n",
    "\n",
    "\tpred_label = [1. if x >= 0.5 else 0. for x in pred_label_]\n",
    "\tresults = pd.DataFrame({'Pred': pred_label,'Prob':pred_label_.reshape(-1), 'True': testy.reshape(-1)})\n",
    "\n",
    "\n",
    "\n",
    "\tif save_model:\n",
    "\t\tmodel.save('{}/{}.keras'.format(model_path,file_path))\n",
    "\n",
    "\treturn accuracy, history\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\t#print(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\t#print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\treturn m\n",
    "\n",
    "results = {\"File\":[],\n",
    "\t\t   \"Scores\":[],\n",
    "\t\t   \"Best_Accuracy\":[]}\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=1):\n",
    "\tsave_model = 1\n",
    "\tmodel_path = '../models/1st_phase'\n",
    "\n",
    "\tdirectory = \"../generated_data/training_folders\"\n",
    "\tfor file_path in tqdm(os.listdir(directory)):\n",
    "\t\tdataset_path = directory + \"/\" +file_path\n",
    "\t\t\t\n",
    "\t\t#load data\n",
    "\t\ttrainX, trainy = load_full_dataset('train',dataset_path)\n",
    "\t\ttestX, testy = load_full_dataset('test',dataset_path)\n",
    "\t\t#print(trainX.shape,trainy.shape,testX.shape,testy.shape)\n",
    "\t\t\n",
    "\t\t# repeat experiment\n",
    "\t\tscores = list()\n",
    "\t\tfor r in range(repeats):\n",
    "\t\t\tscore,history = evaluate_model_2dconv(trainX, trainy, testX, testy, save_model, model_path, file_path)\n",
    "\n",
    "\t\t\tscore = score * 100.0\n",
    "\t\t\t#print('>#%d: %.3f' % (r+1, score))\n",
    "\t\t\tscores.append(score)\n",
    "\n",
    "\t\t#print(score)\n",
    "\t\tbest_score = summarize_results(scores)\n",
    "\t\t\n",
    "\t\t# save it to dataframe\n",
    "\t\tresults[\"File\"].append(file_path)\n",
    "\t\tresults[\"Scores\"].append(scores)\n",
    "\t\tresults[\"Best_Accuracy\"].append(best_score)\n",
    "\t\tdf = pd.DataFrame(results)\n",
    "\t\tmax_accuracy_row = df.loc[df['Best_Accuracy'].idxmax()]\n",
    "\t\tprint(max_accuracy_row)\n",
    "\t\tdf.to_csv(\"../static/phase_1_results.csv\", header=False)\n",
    "\n",
    "run_experiment()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=1):\n",
    "\tsave_model = 1\n",
    "\tmodel_path = '../models/1st_phase'\n",
    "\n",
    "\tdirectory = \"../generated_data/training_folders\"\n",
    "\tfor file_path in tqdm(os.listdir(directory)):\n",
    "\t\tdataset_path = directory + \"/\" +file_path\n",
    "\t\t\t\n",
    "\t\t#load data\n",
    "\t\ttrainX, trainy = load_full_dataset('train',dataset_path)\n",
    "\t\ttestX, testy = load_full_dataset('test',dataset_path)\n",
    "\t\t#print(trainX.shape,trainy.shape,testX.shape,testy.shape)\n",
    "\t\t\n",
    "\t\t# repeat experiment\n",
    "\t\tscores = list()\n",
    "\t\tfor r in range(repeats):\n",
    "\t\t\tscore,history = evaluate_model_2dconv(trainX, trainy, testX, testy, save_model, model_path, file_path)\n",
    "\n",
    "\t\t\tscore = score * 100.0\n",
    "\t\t\t#print('>#%d: %.3f' % (r+1, score))\n",
    "\t\t\tscores.append(score)\n",
    "\n",
    "\t\t#print(score)\n",
    "\t\tbest_score = summarize_results(scores)\n",
    "\t\t\n",
    "\t\t# save it to dataframe\n",
    "\t\tresults[\"File\"].append(file_path)\n",
    "\t\tresults[\"Scores\"].append(scores)\n",
    "\t\tresults[\"Best_Accuracy\"].append(best_score)\n",
    "\t\tdf = pd.DataFrame(results)\n",
    "\t\tmax_accuracy_row = df.loc[df['Best_Accuracy'].idxmax()]\n",
    "\t\tprint(max_accuracy_row)\n",
    "\t\tdf.to_csv(\"../static/phase_1_results.csv\", header=False)\n",
    "\n",
    "run_experiment()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 28 training phases..\n",
    "\n",
    "what we can do is.. we train the model_1 with 1st Hr file..\n",
    "\n",
    "Then use that model architecture as transfer learning to train the 2nd..\n",
    "\n",
    "Following this patter we can train the data over multiple data sources to extract best of all the Hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_minmax_file(dataset_path, type_, data_type, apply_noise=False, scaler=None):\n",
    "    \"\"\"\n",
    "    Reads a .txt file with numerical data line-by-line.\n",
    "    \n",
    "    For training (scaler is None):\n",
    "        - Optionally adds Gaussian noise (σ=0.4).\n",
    "        - Computes the min and max values of the data and then applies Min–Max scaling.\n",
    "        - Returns the scaled matrix and a dictionary with:\n",
    "              'minmax': (min_val, max_val)\n",
    "              \n",
    "    For test data (scaler provided):\n",
    "        - Does not add noise.\n",
    "        - Uses the learned min–max values to perform scaling.\n",
    "    \n",
    "    Returns:\n",
    "        scaled_matrix: The scaled data.\n",
    "        scaler_params: Dictionary with min–max parameters.\n",
    "        valid_indices: List of line indices that were successfully parsed.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_path, type_, f\"{data_type}.txt\")\n",
    "    matrix = []\n",
    "    valid_indices = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                values = [float(x) for x in line.strip().split()]\n",
    "                matrix.append(values)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid line {idx+1} in {file_path}: {e}\")\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    # Add Gaussian noise only for training (increase noise to 0.4)\n",
    "    if apply_noise:\n",
    "        matrix = matrix + np.random.normal(loc=0.0, scale=0.16, size=matrix.shape)\n",
    "    \n",
    "    if scaler is None:\n",
    "        # Compute min and max values for min-max scaling\n",
    "        min_val = matrix.min()\n",
    "        max_val = matrix.max()\n",
    "        if max_val - min_val == 0:\n",
    "            scaled = matrix\n",
    "        else:\n",
    "            scaled = (matrix - min_val) / (max_val - min_val)\n",
    "        scaler_params = {'minmax': (min_val, max_val)}\n",
    "    else:\n",
    "        min_val, max_val = scaler['minmax']\n",
    "        if max_val - min_val == 0:\n",
    "            scaled = matrix\n",
    "        else:\n",
    "            scaled = (matrix - min_val) / (max_val - min_val)\n",
    "        scaler_params = scaler  # unchanged for test\n",
    "    \n",
    "    return scaled, scaler_params, valid_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_minmax_hr(dataset_path, type_, apply_noise=False, scaler=None):\n",
    "    \"\"\"\n",
    "    Special processing for hr.txt:\n",
    "      - Skip lines that have exactly 9 columns.\n",
    "      - For valid lines, parse numerical data.\n",
    "    \n",
    "    For training (scaler is None):\n",
    "        - Optionally adds Gaussian noise.\n",
    "        - Computes the min and max values of the data and then applies Min–Max scaling.\n",
    "        - Returns the scaled data and a dictionary with:\n",
    "              'minmax': (min_val, max_val)\n",
    "    \n",
    "    For test data (scaler provided):\n",
    "        - Does not add noise.\n",
    "        - Uses the learned min–max values to perform scaling.\n",
    "    \n",
    "    Returns:\n",
    "        hr_scaled: The scaled HR data.\n",
    "        scaler_params: Dictionary with scaling parameters.\n",
    "        valid_indices: List of valid line indices.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_path, type_, \"hr.txt\")\n",
    "    hr_list = []\n",
    "    valid_indices = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            cols = line.strip().split()\n",
    "            # Skip lines with exactly 9 columns.\n",
    "            if len(cols) == 9:\n",
    "                continue\n",
    "            try:\n",
    "                values = [float(x) for x in cols]\n",
    "                hr_list.append(values)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid HR line {idx+1} in {file_path}: {e}\")\n",
    "    hr = np.array(hr_list)\n",
    "    \n",
    "    if apply_noise:\n",
    "        hr = hr + np.random.normal(loc=0.0, scale=0.16, size=hr.shape)\n",
    "    \n",
    "    if scaler is None:\n",
    "        min_val = hr.min()\n",
    "        max_val = hr.max()\n",
    "        if max_val - min_val == 0:\n",
    "            hr_scaled = hr\n",
    "        else:\n",
    "            hr_scaled = (hr - min_val) / (max_val - min_val)\n",
    "        scaler_params = {'minmax': (min_val, max_val)}\n",
    "    else:\n",
    "        min_val, max_val = scaler['minmax']\n",
    "        if max_val - min_val == 0:\n",
    "            hr_scaled = hr\n",
    "        else:\n",
    "            hr_scaled = (hr - min_val) / (max_val - min_val)\n",
    "        scaler_params = scaler\n",
    "    \n",
    "    return hr_scaled, scaler_params, valid_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_full_dataset(dataset_path,type_, scalers=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the full dataset.\n",
    "\n",
    "    For each feature (shape, el, dist, hr):\n",
    "      - Reads the file.\n",
    "      - For training: adds Gaussian noise (σ=0.4), computes min–max scaling.\n",
    "      - For test: uses the learned min–max scaling parameters from training.\n",
    "    Computes the common valid indices and combines the features.\n",
    "    \n",
    "    Returns:\n",
    "        data_X: Combined 4D array (samples, features, timesteps, channels).\n",
    "        classification: Filtered classification labels.\n",
    "        scalers: Dictionary of scaling parameters for each feature (for training) or None (for test).\n",
    "    \"\"\"\n",
    "    # Load classification labels.\n",
    "    class_file = os.path.join(dataset_path, type_, \"classification.txt\")\n",
    "    classification = np.loadtxt(class_file).reshape(-1, 1)\n",
    "    classification_indices = list(range(len(classification)))\n",
    "    \n",
    "    # Use noise only for training.\n",
    "    apply_noise_flag = True if type_ == 'train' else False\n",
    "    \n",
    "    # Process HR using its special handler.\n",
    "    if scalers is None or 'hr' not in scalers:\n",
    "        hr_scaled, hr_scaler, hr_valid = read_and_minmax_hr(dataset_path, type_, apply_noise=apply_noise_flag, scaler=None)\n",
    "    else:\n",
    "        hr_scaled, hr_scaler, hr_valid = read_and_minmax_hr(dataset_path, type_, apply_noise=False, scaler=scalers['hr'])\n",
    "    \n",
    "    # Process other features: shape, el, dist.\n",
    "    feat_names = ['shape', 'el', 'dist']\n",
    "    feature_data = {}\n",
    "    feature_scalers = {}\n",
    "    feature_valid = {}\n",
    "    for feat in feat_names:\n",
    "        if scalers is None or feat not in scalers:\n",
    "            data, scaler_param, valid_idx = read_and_minmax_file(dataset_path, type_, feat, apply_noise=apply_noise_flag, scaler=None)\n",
    "        else:\n",
    "            data, scaler_param, valid_idx = read_and_minmax_file(dataset_path, type_, feat, apply_noise=False, scaler=scalers[feat])\n",
    "        feature_data[feat] = data\n",
    "        feature_scalers[feat] = scaler_param\n",
    "        feature_valid[feat] = valid_idx\n",
    "    \n",
    "    # Determine common valid indices among all features and classification.\n",
    "    common_valid = set(classification_indices)\n",
    "    common_valid = common_valid.intersection(set(hr_valid))\n",
    "    for feat in feat_names:\n",
    "        common_valid = common_valid.intersection(set(feature_valid[feat]))\n",
    "    common_valid = sorted(common_valid)\n",
    "    if len(common_valid) == 0:\n",
    "        raise ValueError(\"No common valid indices across all features!\")\n",
    "    \n",
    "    # Filter each array using the common valid indices.\n",
    "    classification = classification[common_valid]\n",
    "    hr_scaled = hr_scaled[common_valid]\n",
    "    for feat in feat_names:\n",
    "        feature_data[feat] = feature_data[feat][common_valid]\n",
    "\n",
    "    # Validate shapes: all features must match in sample count and timesteps.\n",
    "    def validate_feature_shapes(features_dict):\n",
    "        first_shape = None\n",
    "        #print(type_)\n",
    "        for name, arr in features_dict.items():\n",
    "            #print(name,arr.shape)\n",
    "            if first_shape is None:\n",
    "                first_shape = (arr.shape[0], arr.shape[1])\n",
    "            else:\n",
    "                if (arr.shape[0], arr.shape[1]) != first_shape:\n",
    "                    raise ValueError(f\"Shape mismatch in {name}: Expected {first_shape}, got {(arr.shape[0], arr.shape[1])}\")\n",
    "    validate_feature_shapes({feat: feature_data[feat] for feat in feat_names})\n",
    "    validate_feature_shapes({'hr': hr_scaled, **{feat: feature_data[feat] for feat in feat_names}})\n",
    "    \n",
    "    # Combine features along a new axis.\n",
    "    # New order: (shape, dist, el, hr)\n",
    "    data_X = np.stack([feature_data['shape'], feature_data['dist'], feature_data['el'], hr_scaled], axis=1)\n",
    "    # Reshape for CNN input: (samples, features, timesteps, channels)\n",
    "    data_X = data_X.reshape(data_X.shape[0], data_X.shape[1], data_X.shape[2], 1)\n",
    "    \n",
    "    # For training data, return the computed scalers; for test, return None.\n",
    "    if scalers is None:\n",
    "        scalers_out = {\n",
    "            'hr': hr_scaler,\n",
    "            'shape': feature_scalers['shape'],\n",
    "            'el': feature_scalers['el'],\n",
    "            'dist': feature_scalers['dist']\n",
    "        }\n",
    "    else:\n",
    "        scalers_out = None\n",
    "    return data_X, classification, scalers_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# CNN Model Definition\n",
    "#########################################\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=9, kernel_size=(4,1), input_shape=input_shape,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters=3, kernel_size=(1,3), activation='relu')) #(1,5)\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_optimal_threshold(model, X_val, y_val):\n",
    "    \"\"\"Find optimal threshold using ROC curve analysis\"\"\"\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\n",
    "    # Calculate Youden's J statistic\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    # Calculate AUC\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "    return optimal_threshold, fpr, tpr, roc_auc\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculate all metrics for a given threshold\"\"\"\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    metrics = {\n",
    "        'confusion_matrix': cm,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': tp/(tp+fp) if (tp+fp) > 0 else 0,\n",
    "        'recall': tp/(tp+fn) if (tp+fn) > 0 else 0,\n",
    "        'f1': 2*tp/(2*tp + fp + fn) if (2*tp + fp + fn) > 0 else 0,\n",
    "        'accuracy': (tp + tn)/(tp + tn + fp + fn),\n",
    "        'threshold': threshold,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc, threshold,file_name=\"test\"):\n",
    "    \"\"\"Plot ROC curve with optimal threshold marker\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.scatter(fpr[np.argmax(tpr - fpr)], tpr[np.argmax(tpr - fpr)], \n",
    "                color='red', label=f'Optimal Threshold ({threshold:.3f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'../static/roc_curves/phase_1/{file_name}.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7252d58bc91e450ab5ea25fddcb7fdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment():\n",
    "    save_model = 1\n",
    "    model_path = '../models/1st_phase'\n",
    "\n",
    "    result_file_path = '../static/phase_1_results_32.csv'\n",
    "    directory_ = \"../Archive/generated_data_achive/training_folders\"\n",
    "    #directory = \"../dataset\"\n",
    "    # start the for loop for filename and string concat for the complete path\n",
    "    #directory_ = \"../generated_data/training_folders\"\n",
    "    for file_path in tqdm(os.listdir(directory_)):\n",
    "        dataset_path = directory_ + \"/\" +file_path\n",
    "    \n",
    "\n",
    "        try:\n",
    "            # Load and split data\n",
    "            trainX, trainy, scalers = load_full_dataset(dataset_path,'train')\n",
    "            #print(trainX.shape,trainy.shape)\n",
    "            # Split training data into train/validation\n",
    "            split_idx = int(len(trainX) * 0.8)\n",
    "            X_train, X_val = trainX[:split_idx], trainX[split_idx:]\n",
    "            y_train, y_val = trainy[:split_idx], trainy[split_idx:]\n",
    "            \n",
    "            # Build and train model\n",
    "            model = build_model(trainX.shape[1:])\n",
    "            es = EarlyStopping(monitor='val_accuracy', patience=200, restore_best_weights=True)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=300,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_val, y_val),\n",
    "                callbacks=[es],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            if save_model:\n",
    "                model.save('{}/{}.keras'.format(model_path, file_path))\n",
    "            \n",
    "            # Find optimal threshold using ROC analysis\n",
    "            optimal_threshold, fpr, tpr, roc_auc = find_optimal_threshold(model, X_val, y_val)\n",
    "            plot_roc_curve(fpr, tpr, roc_auc, optimal_threshold,file_path)\n",
    "            \n",
    "            # Load and evaluate test data\n",
    "            testX, testy, _ = load_full_dataset(dataset_path,'test', scalers)\n",
    "        \n",
    "            y_test_pred = model.predict(testX)\n",
    "            \n",
    "            # Calculate metrics with optimal threshold\n",
    "            test_metrics = calculate_metrics(testy, y_test_pred, optimal_threshold)\n",
    "            # Print comprehensive results\n",
    "            # make test_metrics[\"confusion_matrix\"] as string\n",
    "            test_metrics[\"File\"] = file_path\n",
    "            test_metrics[\"confusion_matrix\"] = str(test_metrics[\"confusion_matrix\"]) \n",
    "            test_metrics = {key: [value] for key, value in test_metrics.items()}\n",
    "\n",
    "            result_df = pd.DataFrame(test_metrics)\n",
    "\n",
    "            #print(result_df.info())\n",
    "            # Check if the file exists\n",
    "            if not os.path.exists(result_file_path):\n",
    "                # If it doesn't exist, save the current DataFrame\n",
    "                result_df.to_csv(result_file_path, index=False)\n",
    "            else:\n",
    "                # If it exists, read the existing file\n",
    "                existing_df = pd.read_csv(result_file_path)\n",
    "                \n",
    "                # Append the current DataFrame to the existing one\n",
    "                combined_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "                \n",
    "                # Save the combined DataFrame back to the file\n",
    "                combined_df.to_csv(result_file_path, index=False)\n",
    "        \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusion_matrix    [[505 147]\\n [135 495]]\n",
       "tp                                      495\n",
       "tn                                      505\n",
       "fp                                      147\n",
       "fn                                      135\n",
       "precision                          0.771028\n",
       "recall                             0.785714\n",
       "f1                                 0.778302\n",
       "accuracy                           0.780031\n",
       "threshold                          0.503254\n",
       "roc_auc                            0.851195\n",
       "File                 L_hydrophobicity_scale\n",
       "Name: 51, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../static/phase_1_results_32.csv')\n",
    "# Find the row with the highest accuracy\n",
    "top_accuracy_row = df.loc[df['accuracy'].idxmax()]\n",
    "\n",
    "top_accuracy_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusion_matrix    [[514 138]\\n [149 481]]\n",
       "tp                                      481\n",
       "tn                                      514\n",
       "fp                                      138\n",
       "fn                                      149\n",
       "precision                           0.77706\n",
       "recall                             0.763492\n",
       "f1                                 0.770216\n",
       "accuracy                           0.776131\n",
       "threshold                          0.506026\n",
       "roc_auc                            0.853717\n",
       "File                             KUHL950101\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../static/phase_2_results_32.csv')\n",
    "# Find the row with the highest accuracy\n",
    "top_accuracy_row = df.loc[df['accuracy'].idxmax()]\n",
    "\n",
    "top_accuracy_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
