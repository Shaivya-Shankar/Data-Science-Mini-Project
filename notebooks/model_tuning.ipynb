{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install FLAML\n",
    "%pip install scikit-learn\n",
    "import flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt  # Import Keras Tuner\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and testing data\n",
    "\n",
    "# read data from a file and normalize it\n",
    "def read_and_norm(type_,type):\n",
    "    with open('dataset/{}/{}.txt'.format( type_,type), 'rb') as f:\n",
    "        matrix = [[float(x) for x in line.split()] for line in f]\n",
    "    matrix = np.array(matrix)\n",
    "    min_m = matrix.min().min()\n",
    "    max_m = matrix.max().max()\n",
    "    matrix = ((matrix - min_m) / (max_m - min_m))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(hr_file_path,type_):\n",
    "    # read shape, el and dist\n",
    "    shape = read_and_norm(type_,'shape')\n",
    "    el = read_and_norm(type_,'el')\n",
    "    dist = read_and_norm(type_,'dist')\n",
    "\n",
    "    # read labels\n",
    "    classification = np.loadtxt('dataset/{}/classification.txt'.format(type_))\n",
    "    classification = np.array(classification).reshape(-1,1)    \n",
    "\n",
    "    with open(hr_file_path, \"r\") as file:\n",
    "        hr = []\n",
    "        righe_con_9_colonne = []\n",
    "        for indice, riga in enumerate(file):\n",
    "            colonne = riga.split()\n",
    "            if len(colonne) == 9:\n",
    "                righe_con_9_colonne.append(indice)\n",
    "            else:\n",
    "                hr.append(colonne)\n",
    "    hr = [[float(string) for string in inner] for inner in hr]\n",
    "    hr = np.array(hr)\n",
    "    \n",
    "    classification = np.delete(classification, righe_con_9_colonne, 0)\n",
    "    shape = np.delete(shape, righe_con_9_colonne, 0)\n",
    "    el = np.delete(el, righe_con_9_colonne, 0)\n",
    "    dist = np.delete(dist, righe_con_9_colonne, 0)\n",
    "\n",
    "\n",
    "    data_X = np.array([p for p in zip(shape, dist, el, hr)])\n",
    "    data_X = data_X.reshape(data_X.shape[0], data_X.shape[1], data_X.shape[2], 1)\n",
    "\n",
    "    return data_X, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(trainX, trainy, hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('filters_1', min_value=8, max_value=64, step=8),\n",
    "        kernel_size=(4, 1),\n",
    "        activation='relu',\n",
    "        input_shape=(trainX.shape[1], trainX.shape[2], 1)\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('filters_2', min_value=4, max_value=32, step=4),\n",
    "        kernel_size=(1, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_1', min_value=16, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_2', min_value=16, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_4', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(trainy.shape[1], activation='sigmoid'))\n",
    "\n",
    "    lr = hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def best_model(trainX, trainy, testX, testy,save_model = 1, model_path = 'models'):\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        lambda hp: tune_model(trainX,trainy,hp),\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=50,\n",
    "        factor=3,\n",
    "        directory='tuner_results',\n",
    "        project_name='cnn_autotune'\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    tuner.search(trainX, trainy, epochs=50, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    history = best_model.fit(trainX, trainy, epochs=300, batch_size=1, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\t# evaluate model\n",
    "    _, accuracy = best_model.evaluate(testX, testy, batch_size=1, verbose=1)\n",
    "\n",
    "    pred_label_ = best_model.predict(testX,batch_size=1)\n",
    "\n",
    "    pred_label = [1. if x >= 0.5 else 0. for x in pred_label_]\n",
    "    results = pd.DataFrame({'Pred': pred_label,'Prob':pred_label_.reshape(-1), 'True': testy.reshape(-1)})\n",
    "\n",
    "\n",
    "    if save_model:\n",
    "        model_saved_path = '{}/my_model.keras'.format(model_path)\n",
    "        best_model.save('{}/my_model.keras'.format(model_path))\n",
    "\n",
    "    return accuracy, history, model_saved_path\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model_2dconv(trainX, trainy, testX, testy,save_model = 1, model_path = 'models'):\n",
    "\tverbose, epochs, batch_size = 1, 3, 1\n",
    "\tn_outputs = trainy.shape[1]\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Conv2D(filters=9, kernel_size=(4,1), input_shape=trainX.shape[1:],activation='relu'))\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\tmodel.add(Conv2D(filters=3, kernel_size=(1,3), activation='relu')) #(1,5)\n",
    "\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(30, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
    "\tmodel.summary()\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\tes = EarlyStopping(monitor='val_accuracy',\n",
    "\t\t\t\t   mode='max',\n",
    "\t\t\t\t   patience=50,\n",
    "\t\t\t\t   restore_best_weights=True)\n",
    "\n",
    "\thistory = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,validation_split=.2,callbacks=[es])#,callbacks=rlronp)#) #,callbacks=[es])\n",
    "\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\tpred_label_ = model.predict(testX,batch_size=batch_size)\n",
    "\n",
    "\tpred_label = [1. if x >= 0.5 else 0. for x in pred_label_]\n",
    "\tresults = pd.DataFrame({'Pred': pred_label,'Prob':pred_label_.reshape(-1), 'True': testy.reshape(-1)})\n",
    "\n",
    "\n",
    "\n",
    "\tif save_model:\n",
    "\t\tmodel_saved_path = '{}/my_model.keras'.format(model_path)\n",
    "\t\tmodel.save('{}/my_model.keras'.format(model_path))\n",
    "\n",
    "\treturn accuracy, history, model_saved_path\n",
    "\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\t# print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\treturn m,s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X, train_Y,test_X, test_Y):\n",
    "\t\n",
    "\trepeats = 4\n",
    "    # repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\t#score,history,model_saved_path = evaluate_model_2dconv(train_X, train_Y, test_X, test_Y)\n",
    "\t\tscore,history,model_saved_path = tune_model(train_X, train_Y, test_X, test_Y)\n",
    "\t\tscore = score * 100.0\n",
    "\t\t#print('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\n",
    "\tmean,std_dev = summarize_results(scores)\n",
    "    # it should return the saved model path and accuracy\n",
    "\treturn mean,model_saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2990, 4, 10, 1) (2990, 1)\n",
      "(1282, 4, 10, 1) (1282, 1)\n",
      "Reloading Tuner from tuner_results/cnn_autotune/tuner0.json\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 553us/step - accuracy: 0.6618 - loss: 0.6120 - val_accuracy: 0.7809 - val_loss: 0.4678\n",
      "Epoch 2/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525us/step - accuracy: 0.7811 - loss: 0.4781 - val_accuracy: 0.7391 - val_loss: 0.4804\n",
      "Epoch 3/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 520us/step - accuracy: 0.7887 - loss: 0.4695 - val_accuracy: 0.7559 - val_loss: 0.4849\n",
      "Epoch 4/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522us/step - accuracy: 0.7617 - loss: 0.4930 - val_accuracy: 0.7776 - val_loss: 0.4901\n",
      "Epoch 5/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 523us/step - accuracy: 0.8040 - loss: 0.4618 - val_accuracy: 0.7943 - val_loss: 0.4461\n",
      "Epoch 6/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570us/step - accuracy: 0.7928 - loss: 0.4641 - val_accuracy: 0.7793 - val_loss: 0.4417\n",
      "Epoch 7/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525us/step - accuracy: 0.8079 - loss: 0.4473 - val_accuracy: 0.7458 - val_loss: 0.5239\n",
      "Epoch 8/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 520us/step - accuracy: 0.7897 - loss: 0.4582 - val_accuracy: 0.8094 - val_loss: 0.4397\n",
      "Epoch 9/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525us/step - accuracy: 0.7881 - loss: 0.4490 - val_accuracy: 0.7910 - val_loss: 0.4358\n",
      "Epoch 10/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 523us/step - accuracy: 0.8119 - loss: 0.4450 - val_accuracy: 0.7977 - val_loss: 0.4364\n",
      "Epoch 11/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535us/step - accuracy: 0.8106 - loss: 0.4298 - val_accuracy: 0.7692 - val_loss: 0.4774\n",
      "Epoch 12/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531us/step - accuracy: 0.8080 - loss: 0.4403 - val_accuracy: 0.7876 - val_loss: 0.4452\n",
      "Epoch 13/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547us/step - accuracy: 0.8136 - loss: 0.4172 - val_accuracy: 0.7860 - val_loss: 0.4354\n",
      "Epoch 14/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - accuracy: 0.8163 - loss: 0.4223 - val_accuracy: 0.7776 - val_loss: 0.4543\n",
      "Epoch 15/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521us/step - accuracy: 0.8025 - loss: 0.4325 - val_accuracy: 0.7910 - val_loss: 0.4311\n",
      "Epoch 16/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531us/step - accuracy: 0.8162 - loss: 0.4045 - val_accuracy: 0.7977 - val_loss: 0.4308\n",
      "Epoch 17/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521us/step - accuracy: 0.8017 - loss: 0.4287 - val_accuracy: 0.7977 - val_loss: 0.4343\n",
      "Epoch 18/300\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519us/step - accuracy: 0.8250 - loss: 0.4141 - val_accuracy: 0.8010 - val_loss: 0.4306\n",
      "\u001b[1m1282/1282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - accuracy: 0.6733 - loss: 0.6394\n",
      "\u001b[1m1282/1282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203us/step\n",
      "0.672386884689331\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(hr_file_path):\n",
    "    best_model_accuracy = 0\n",
    "    best_model_path = None\n",
    "    # get training data\n",
    "    type_ = \"train\"\n",
    "    train_X, train_Y = _get_data(hr_file_path,type_)\n",
    "    \n",
    "    type_ = \"test\"\n",
    "    test_X, test_Y = _get_data(hr_file_path,type_)\n",
    "\n",
    "    print(train_X.shape, train_Y.shape)\n",
    "    print(test_X.shape, test_Y.shape)\n",
    "    current_model_best_accuracy,history,path = best_model(train_X, train_Y,test_X, test_Y)\n",
    "\n",
    "    if current_model_best_accuracy > best_model_accuracy:\n",
    "        best_model_accuracy = current_model_best_accuracy   \n",
    "        best_model_path = path\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return best_model_accuracy, best_model_path\n",
    "\n",
    "\n",
    "\n",
    "#get_train_test_data(\"datatset_new/train/hr_ARGP820101.txt\")\n",
    "best_model_accuracy,best_model_path = run_experiment(\"dataset/train/hr.txt\")\n",
    "print(best_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to train using AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2990, 4, 10, 1) (2990, 1)\n",
      "(1282, 4, 10, 1) (1282, 1)\n"
     ]
    }
   ],
   "source": [
    "type_ = \"train\"\n",
    "hr_file_path = \"dataset/train/hr.txt\"\n",
    "train_X, train_Y = _get_data(hr_file_path,type_)\n",
    "\n",
    "type_ = \"test\"\n",
    "test_X, test_Y = _get_data(hr_file_path,type_)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print(test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: FLAML\n",
      "Version: 2.3.4\n",
      "Summary: A fast library for automated machine learning and tuning\n",
      "Home-page: https://github.com/microsoft/FLAML\n",
      "Author: Microsoft Corporation\n",
      "Author-email: hpo@microsoft.com\n",
      "License: \n",
      "Location: /Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages\n",
      "Requires: NumPy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show flaml\n",
    "import flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: FLAML==2.1.2 in /Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: NumPy>=1.17 in /Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages (from FLAML==2.1.2) (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install FLAML==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoML' from 'flaml.automl' (/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/flaml/automl/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflaml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoML\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoML' from 'flaml.automl' (/Users/pulkit/Desktop/mini_project/miniproject/lib/python3.12/site-packages/flaml/automl/__init__.py)"
     ]
    }
   ],
   "source": [
    "from flaml.automl import AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
